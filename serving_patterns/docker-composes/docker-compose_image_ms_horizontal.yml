version: "3"

services:
  proxy_ms_h:
    container_name: serving_patterns_proxy_ms_h
    build:
      context: ..
      dockerfile: src/api_composition_proxy/Dockerfile_proxy
    environment:
      - RUNNER=GUNICORN
      - WORKERS=2
      - BACKLOG=128
      - LIMIT_MAX_REQUESTS=2048
      - MAX_REQUESTS_JITTER=256
      - APP_NAME=src.api_composition_proxy.apps.proxy:app
      - PORT=8000
      - PROFILE=0
      - SERVICE_RESNET50=api_resnet50_ms_h:8891
      - SERVICE_MOBILENETV2=api_mobilenetv2_ms_h:8893
      - QUEUE_NAME=api_queue
    ports:
      - "8000:8000"
    command: ./run_proxy.sh

  api_resnet50_ms_h:
    container_name: serving_patterns_api_resnet50_ms_h
    build:
      context: ..
      dockerfile: src/app/ml/resnet50_onnx/Dockerfile_api_resnet50_onnx
    environment:
      - PLATFORM=docker_compose
      - MODEL_INTERFACE=/serving_patterns/src/app/ml/resnet50_onnx/model/resnet50_onnx.yaml
      - RUNNER=GUNICORN
      - WORKERS=4
      - BACKLOG=64
      - LIMIT_MAX_REQUESTS=1024
      - MAX_REQUESTS_JITTER=128
      - APP_NAME=src.app.apps.app_image:app
      - PORT=8891
      - PROFILE=0
      - QUEUE_NAME=resnet50_queue
    ports:
      - "8891:8891"
    command: ./run_api.sh
    depends_on:
      - backend_resnet50_ms_h
      - redis

  backend_resnet50_ms_h:
    container_name: serving_patterns_backend_resnet50_ms_h
    build:
      context: ..
      dockerfile: src/app/ml/resnet50_onnx/Dockerfile_backend_resnet50_onnx
    environment:
      - PLATFORM=docker_compose
      - MODEL_INTERFACE=/serving_patterns/src/app/ml/resnet50_onnx/model/resnet50_onnx.yaml
      - NUM_PROCS=4
      - BATCH_CODE=src.app.backend.prediction_batch
      - PROFILE=0
      - PREDICTOR=src.app.api._predict_image
      - QUEUE_NAME=resnet50_queue
    command: ./run_backend.sh
    depends_on:
      - redis

  api_mobilenetv2_ms_h:
    container_name: serving_patterns_api_mobilenetv2_ms_h
    build:
      context: ..
      dockerfile: src/app/ml/mobilenetv2/Dockerfile_api_mobilenetv2
    environment:
      - PLATFORM=docker_compose
      - MODEL_INTERFACE=/serving_patterns/src/app/ml/mobilenetv2/model/mobilenetv2.yaml
      - RUNNER=GUNICORN
      - WORKERS=4
      - BACKLOG=64
      - LIMIT_MAX_REQUESTS=1024
      - MAX_REQUESTS_JITTER=128
      - APP_NAME=src.app.apps.app_image:app
      - PORT=8893
      - TFS_GRPC=tfs_mobilenetv2_ms_h:8512
      - PROFILE=0
      - QUEUE_NAME=mobilenetv2_queue
    ports:
      - "8893:8893"
    command: ./run_api.sh
    depends_on:
      - redis
      - tfs_mobilenetv2_ms_h
      - backend_mobilenetv2_ms_h

  tfs_mobilenetv2_ms_h:
    container_name: serving_patterns_tfs_mobilenetv2_ms_h
    build:
      context: ..
      dockerfile: src/app/ml/mobilenetv2/Dockerfile_tfserving_mobilenetv2
    environment:
      - PORT=8512
      - REST_API_PORT=8513
      - MODEL_NAME=mobilenetv2
      - MODEL_BASE_PATH=/serving_patterns/src/app/ml/mobilenetv2/model/savedmodel/mobilenetv2
    ports:
      - "8512:8512"
      - "8513:8513"
    entrypoint: ["/usr/bin/tf_serving_entrypoint.sh"]

  backend_mobilenetv2_ms_h:
    container_name: serving_patterns_backend_mobilenetv2_ms_h
    build:
      context: ..
      dockerfile: src/app/ml/mobilenetv2/Dockerfile_backend_mobilenetv2
    environment:
      - PLATFORM=docker_compose
      - MODEL_INTERFACE=/serving_patterns/src/app/ml/mobilenetv2/model/mobilenetv2.yaml
      - NUM_PROCS=4
      - BATCH_CODE=src.app.backend.prediction_batch
      - TFS_GRPC=tfs_mobilenetv2_ms_h:8512
      - PROFILE=0
      - PREDICTOR=src.app.api._predict_image
      - QUEUE_NAME=mobilenetv2_queue
    command: ./run_backend.sh
    depends_on:
      - redis

  redis:
    container_name: serving_patterns_redis
    image: "redis:latest"
    ports:
      - "6379:6379"
